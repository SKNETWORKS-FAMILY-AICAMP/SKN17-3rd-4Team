import streamlit as st
import json
import os
import requests
import torch
import faiss
import numpy as np
from dotenv import load_dotenv
from urllib.parse import unquote
from datetime import datetime
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- 1. ì´ˆê¸° ì„¤ì • ---

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ìœ¡ì•„ ë„ìš°ë¯¸ ë´‡", layout="centered")

# API í‚¤ ë¡œë“œ
try:
    HIRA_API_SERVICE_KEY = os.environ["HIRA_API_SERVICE_KEY"]
    KAKAO_API_REST_KEY = os.environ["KAKAO_API_REST_KEY"]
except KeyError as e:
    st.error(f"í•„ìˆ˜ API í‚¤ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    st.stop()

# --- 2. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ---
@st.cache_resource
def load_all_models_and_data():
    """RAGì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    HF_EMBED_REPO_ID = "WOOJINIYA/parentcare-bot-bge-m3"
    HF_LLM_REPO_ID = "WOOJINIYA/parentcare-bot-qwen2.5-7b"
    LOCAL_INDEX_PATH = "faiss.index"
    LOCAL_META_PATH = "faiss.meta.json"

    embed_model = SentenceTransformer(HF_EMBED_REPO_ID)
    tokenizer = AutoTokenizer.from_pretrained(HF_LLM_REPO_ID)
    model = AutoModelForCausalLM.from_pretrained(
        HF_LLM_REPO_ID, device_map="auto", torch_dtype=torch.float16, load_in_4bit=True
    )
    index = faiss.read_index(LOCAL_INDEX_PATH)
    with open(LOCAL_META_PATH, "r", encoding="utf-8") as f:
        meta = json.load(f)
    return embed_model, tokenizer, model, index, meta["texts"], meta.get("metas")

with st.spinner("AI ëª¨ë¸ê³¼ ìœ¡ì•„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
    embed_model, tokenizer, model, index, TEXTS, METAS = load_all_models_and_data()


# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---

def generate_text(prompt_text, max_tokens, temperature=0.1):
    """LLMì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    inputs = tokenizer(prompt_text, return_tensors="pt", truncation=True, max_length=4096).to(model.device)
    outputs = model.generate(
        **inputs, max_new_tokens=max_tokens, do_sample=True, temperature=temperature, top_p=0.95
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def parse_intent_with_ai(user_prompt):
    """ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ 'search' ë˜ëŠ” 'chat'ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    search_keywords = ['ì£¼ë³€', 'ì°¾ì•„ì¤˜', 'ì–´ë””', 'ì†Œì•„ê³¼', 'ì‚°ë¶€ì¸ê³¼', 'ì•½êµ­', 'ë³‘ì›']
    if any(keyword in user_prompt for keyword in search_keywords):
        system_prompt = "ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì—ì„œ 'place'ì™€ 'location'ì„ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ë¼. 'place'ëŠ” ë³‘ì›, ì•½êµ­, ì†Œì•„ê³¼, ì‚°ë¶€ì¸ê³¼ ë“± ì¥ì†Œ ì¢…ë¥˜ë¥¼ ì˜ë¯¸í•˜ê³  'location'ì€ 'ì„œìš¸ ëŒ€ë°©ë™' ê°™ì€ ì§€ì—­ëª…ì„ ì˜ë¯¸í•œë‹¤. ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ nullë¡œ í‘œì‹œí•´ë¼. ì˜ˆì‹œ: {\"place\": \"ì†Œì•„ê³¼\", \"location\": \"ì„œìš¸ ëŒ€ë°©ë™\"}"
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        full_response = generate_text(prompt, 128)
        try:
            json_part = full_response[full_response.find('{'):full_response.rfind('}')+1]
            intent = json.loads(json_part)
            intent['action'] = 'search'
            return intent
        except Exception:
            return {"action": "chat"}
    return {"action": "chat"}

# --- ì¥ì†Œ ê²€ìƒ‰ (Kakao & HIRA API) ê´€ë ¨ í•¨ìˆ˜ ---
def geocode_kakao(address):
    endpoint = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_REST_KEY}"}
    params = {'query': address}
    try:
        response = requests.get(endpoint, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data['documents']:
            doc = data['documents'][0]
            return float(doc['y']), float(doc['x']), None
        return None, None, f"'{address}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return None, None, f"ì¹´ì¹´ì˜¤ë§µ API ì˜¤ë¥˜: {e}"

def _search_hira(lat, lon, place_type, endpoint):
    params = {"serviceKey": unquote(HIRA_API_SERVICE_KEY), "xPos": lon, "yPos": lat, "radius": 3000, "_type": "json"}
    try:
        response = requests.get(endpoint, params=params, timeout=10)
        response.raise_for_status()
        items = response.json().get('response', {}).get('body', {}).get('items', {}).get('item', [])
        if not items: return f"ì£¼ë³€ 3km ì´ë‚´ì— ê²€ìƒ‰ëœ '{place_type}'ì´(ê°€) ì—†ìŠµë‹ˆë‹¤."
        
        sorted_items = sorted(items, key=lambda x: float(x['distance']))
        today_weekday = datetime.now().weekday() + 1
        
        response_text = f"âœ… **'{place_type}'** ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. (ê±°ë¦¬ ìˆœ, ìµœëŒ€ 5ê³³)\n\n"
        for item in sorted_items[:5]:
            response_text += f"**{item['yadmNm']}**\n- ì£¼ì†Œ: {item['addr']}\n"
            telno = item.get('telno', 'ì •ë³´ ì—†ìŒ')
            tel_link = ''.join(filter(str.isdigit, telno)) if telno else ''
            response_text += f"- ì „í™”: [{telno}](tel:{tel_link})\n"
            start_time, close_time = item.get(f'dutyTime{today_weekday}s'), item.get(f'dutyTime{today_weekday}c')
            operating_hours = f"{start_time[:2]}:{start_time[2:]} ~ {close_time[:2]}:{close_time[2:]}" if start_time and close_time else "ì •ë³´ ì—†ìŒ"
            response_text += f"- ì˜¤ëŠ˜ ì˜ì—…ì‹œê°„: {operating_hours} (ë°©ë¬¸ ì „ í™•ì¸ í•„ìˆ˜)\n---\n"
        return response_text
    except Exception as e:
        return f"{place_type} API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def search_hospitals(lat, lon, place_type):
    return _search_hira(lat, lon, place_type, "http://apis.data.go.kr/B551182/hospInfoServicev2/getHospBasisList")
def search_pharmacies(lat, lon):
    return _search_hira(lat, lon, "ì•½êµ­", "http://apis.data.go.kr/B551182/pharmacyInfoService/getParmacyBasisList")

def handle_search(intent):
    location_name = intent.get("location")
    place_type = intent.get("place", "ë³‘ì›")
    if not location_name: return "ì–´ë”” ì£¼ë³€ì—ì„œ ì°¾ì•„ë“œë¦´ê¹Œìš”? 'ì„œìš¸ ëŒ€ë°©ë™'ì²˜ëŸ¼ ì§€ì—­ ì´ë¦„ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    lat, lon, error_message = geocode_kakao(location_name)
    if error_message: return f"âš ï¸ ìœ„ì¹˜ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n\n`{error_message}`"
    if lat and lon:
        return search_pharmacies(lat, lon) if "ì•½êµ­" in place_type else search_hospitals(lat, lon, place_type)

# --- RAG ê¸°ë°˜ ìœ¡ì•„ ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜ ---
SYSTEM_PROMPT_RAG = """
ë‹¹ì‹ ì€ í•œêµ­ ë¶€ëª¨ë¥¼ ë•ëŠ” ìœ¡ì•„ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹(ì •í™•íˆ ì´ 4ê°œ ì„¹ì…˜ë§Œ, ê° ì„¹ì…˜ ê·œì¹™ ì—„ìˆ˜):
1) âœ… í•µì‹¬ ìš”ì•½:
      ë¬¸ì¥ 3ê°œ ì´ë‚´.

2) ğŸ‘¶ ë‹¨ê³„ë³„ ê°€ì´ë“œ:
      ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ 3~10ê°œ. ê° í•­ëª©ì€ í•œ ë¬¸ì¥.

3) ğŸ“Œ ê·¼ê±°:
      ê° ì¤„ì— (id / title / category) ë§Œì•½ì— ë˜‘ê°™ì€ ì¶œì²˜ê°€ ìˆë‹¤ë©´ 1ê°œì”©ë§Œ ì¶œë ¥í•´.

4) âš ï¸ ì£¼ì˜/ë©´ì±…:
      ë¬¸ì¥ 2ê°œ ì´ë‚´. ê°™ì€ ë¬¸ì¥ì„ ë°˜ë³µ ê¸ˆì§€.

ê·œì¹™:
- ì–´ë–¤ ì§ˆë³‘ì´ ì˜ì‹¬ë˜ë©´ ì§ˆë³‘ëª…ë„ í•¨ê»˜ ì œì‹œí•œë‹¤.
- ì˜í•™/ì •ì±… ì •ë³´ëŠ” ë°˜ë“œì‹œ ê·¼ê±°(ê¸°ê´€ëª…+ë‚ ì§œ+URL)ë¥¼ í•¨ê»˜ ì œì‹œí•œë‹¤.
- ê·¼ê±° ë¶ˆëª…í™• ì‹œ 'í™•ì¸ í•„ìš”'ë¼ê³ ë§Œ ì“°ê³ , ì¼ë°˜ì  ì£¼ì˜ì‚¬í•­ì„ ê°„ê²°íˆ ì œì‹œí•œë‹¤.
- ì‘ê¸‰ ì§•í›„(ê³ ì—´, í˜¸í¡ê³¤ë€, ê²½ë ¨, íƒˆìˆ˜ ë“±) ì–¸ê¸‰ ì‹œ 119/ì‘ê¸‰ì‹¤ ì•ˆë‚´ë¥¼ í¬í•¨í•œë‹¤.
- ê°™ì€ ë¬¸ì¥/ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€. ì´ë¯¸ ë§í•œ ë‚´ìš© ì¬ì„œìˆ  ê¸ˆì§€.
- ìœ„ 4ê°œ ì„¹ì…˜ ì¶œë ¥ í›„ ì¦‰ì‹œ ì¢…ë£Œ(ì¶”ê°€ ë¬¸ì¥ ê¸ˆì§€).
- ë‹µë³€ í˜•ì‹ ì™¸ì— ì–´ë–¤ ë¶€ê°€ì ì¸ ì„¤ëª…, ì£¼ì„, íƒœê·¸ë„ ì ˆëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
"""

def retrieve(query, top_k=5):
    q_emb = embed_model.encode([query], normalize_embeddings=True).astype("float32")
    _, indices = index.search(q_emb, top_k)
    return [{"meta": METAS[i], "text": TEXTS[i]} for i in indices[0]]

def build_context(docs):
    context = []
    for i, doc in enumerate(docs, 1):
        meta, text = doc["meta"], doc["text"]
        context.append(f"[{i}] ì¶œì²˜: {meta.get('id', 'N/A')} | ì œëª©: {meta.get('title', 'N/A')}\n{text}")
    return "\n\n".join(context)

def ask_rag(user_prompt):
    docs = retrieve(user_prompt)
    context_str = build_context(docs)
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_RAG},
        {"role": "user", "content": f"ì§ˆë¬¸: {user_prompt}\n\nì•„ë˜ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\n\n{context_str}"}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    full_response = generate_text(prompt, 512, temperature=0.2)
    return full_response[len(prompt)-len(" <|im_start|>assistant\n"):] # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œì™¸

# --- 4. Streamlit UI ë° ë©”ì¸ ë¡œì§ ---
st.title("AI ìœ¡ì•„ ë„ìš°ë¯¸ ë´‡ ğŸ¤–")
st.image("img.jpg", use_container_width=True)
st.info("ì„ì‹ , ì¶œì‚°, ìœ¡ì•„ ì§ˆë¬¸ì€ ë¬¼ë¡ , 'ë¶€ì²œ ì›ì¢…ë™ ì£¼ë³€ ì†Œì•„ê³¼ ì°¾ì•„ì¤˜'ì²˜ëŸ¼ ì¥ì†Œ ê²€ìƒ‰ë„ ê°€ëŠ¥í•´ìš”!")

# ì‚¬ì´ë“œë°”
st.sidebar.title("ë©”ë‰´")
# ... (ê¸°ì¡´ ì‚¬ì´ë“œë°” ì½”ë“œëŠ” ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°)

# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë° í‘œì‹œ
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë° ì±—ë´‡ ì‘ë‹µ ì²˜ë¦¬ ë¡œì§
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.rerun()

if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
    user_prompt = st.session_state.messages[-1]["content"]
    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ì—´ì‹¬íˆ ìƒê°í•˜ê³  ìˆì–´ìš”... ğŸ¤”"):
            intent = parse_intent_with_ai(user_prompt)
            action = intent.get("action", "chat")

            if action == "search":
                response_content = handle_search(intent)
            else: # action == "chat"
                response_content = ask_rag(user_prompt)

    st.session_state.messages.append({"role": "assistant", "content": response_content})
    st.rerun()


# --- CSS ìŠ¤íƒ€ì¼ë§ ë° ì´ë¯¸ì§€ ì˜¤ë²„ë ˆì´ ì½”ë“œ ---

st.markdown("""
<style>
/* === ê¸°ë³¸ í°íŠ¸/ìƒ‰ í† í° === */
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;600;700&display=swap');
:root{
  --brand:#6C7CFF;
  --bg:#F7F9FC;
  --surface:#FFFFFF;
  --line:#E9EEF5;
  --text:#111827;
  --radius:12px;
  --radius-lg:16px;
  --font:'Noto Sans KR', system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
}
@media (prefers-color-scheme: dark){
  :root{ --bg:#0B1220; --surface:#0F1629; --line:#1E2A44; --text:#E5E7EB; }
}
body, div, p, span, h1, h2, h3, h4, h5, h6{ font-family:var(--font); }

/* === ë°°ê²½/ë ˆì´ì•„ì›ƒ ìµœì†Œ === */
.main{
  background:
    radial-gradient(1200px 600px at 10% -10%, rgba(108,124,255,.06) 0%, rgba(108,124,255,0) 50%),
    linear-gradient(180deg, var(--bg) 0%, var(--surface) 64%);
}
section.main > div.block-container{ padding-top:1rem; padding-bottom:1.4rem; }

/* ëª¨ì…˜ ìµœì†Œí™” ì¡´ì¤‘ */
@media (prefers-reduced-motion: reduce){ *{animation:none !important; transition:none !important;} }

/* === íƒ€ì´í‹€ë§Œ ê°€ë³ê²Œ === */
.block-container h1:first-of-type{
  margin-bottom:.35rem; font-weight:800; letter-spacing:-.2px;
  background:linear-gradient(90deg, var(--text), color-mix(in oklab, var(--text), #334155 60%));
  -webkit-background-clip:text; background-clip:text; color:transparent;
}

/* === ì´ë¯¸ì§€ ê³µí†µ(ê·¸ë¦¼ì/í…Œë‘ë¦¬ ì œê±°) === */
[data-testid="stImage"] img{
  display:block; width:100%; height:auto; max-height:42vh; object-fit:cover;
  border-radius:var(--radius-lg); box-shadow:none !important; border:none !important; background:transparent !important;
}

/* === ì±„íŒ… ì…ë ¥ì°½ ìµœì†Œ === */
[data-testid="stChatInput"]{ border-top:1px dashed var(--line); padding-top:.6rem; margin-top:.6rem; }
[data-testid="stChatInput"] textarea{
  border:1px solid var(--line) !important; border-radius:var(--radius) !important; box-shadow:0 2px 10px rgba(0,0,0,.03) !important;
}

/* === ì›Œí„°ë§ˆí¬(ì±„íŒ…ì°½ ìœ„ ê³ ì •) === */
.ux-footer{ color:#6B7280; font-size:12px; text-align:center; margin-top:14px; user-select:none; }
.ux-over-chat{
  position:fixed; left:0; right:0; bottom:72px; padding:.25rem 0; background:transparent; z-index:5; pointer-events:none;
}
@media (max-width: 640px){ .ux-over-chat{ bottom:88px; } }

/* === íˆì–´ë¡œ ì´ë¯¸ì§€ ìœ„ì¹˜(ì¢Œìƒ/ìš°í•˜) === */
.hero-left, .hero-right{
  position:fixed; max-width:none !important; height:auto; object-fit:contain;
  border-radius:12px; box-shadow:none !important; border:none !important; background:transparent !important; pointer-events:none; z-index:0;
}
.hero-left{ top:120px; left:48px; width:220px; }   /* í™”ë©´ ìœ„ìª½ ì™¼ìª½ */
.hero-right{ top:360px; right:48px; width:240px; } /* í™”ë©´ ì˜¤ë¥¸ìª½ ì•„ë˜ */


@media (max-width: 900px){ .hero-left, .hero-right{ display:none; } }

/* === ì‘ì€ í™”ë©´ ë³´ì • === */
@media (max-width: 640px){ [data-testid="stImage"] img{ max-height:32vh; } }
</style>
""", unsafe_allow_html=True)

import base64 as _b64
def _img_to_data_uri(path):
    try:
        with open(path, "rb") as _f:
            return "data:image/png;base64," + _b64.b64encode(_f.read()).decode("utf-8")
    except Exception:
        return ""
_left_uri = _img_to_data_uri("1.png")
_right_uri = _img_to_data_uri("2.png")
if _left_uri:
    st.markdown(f'<img class="hero-left" src="{_left_uri}" alt="left">', unsafe_allow_html=True)
if _right_uri:
    st.markdown(f'<img class="hero-right" src="{_right_uri}" alt="right">', unsafe_allow_html=True)